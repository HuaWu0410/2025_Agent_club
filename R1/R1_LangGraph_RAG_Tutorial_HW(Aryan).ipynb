{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgOU80G6LSnE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langgraph transformers bitsandbytes langchain-huggingface langchain-community chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph"
      ],
      "metadata": {
        "id": "dpqj6aWx_noI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangGraph 是建立在 LangChain 基礎上的進階框架，專門設計來支援 有狀態 (stateful)、多輪互動的 LLM 應用。\n",
        "\n",
        "它最適合開發需要「多步推理」、「分支決策」、「循環執行」的場景，例如智慧助理、複雜的問答流程、知識檢索 (RAG) 系統等等。\n",
        "\n",
        "這個函式庫的核心思想是：以「有向圖 (Directed Graph)」的方式來設計 LLM 應用流程，每個節點 (Node) 是一個執行單位，各節點之間以邊 (Edge) 連結，定義資料如何流動。\n",
        "\n",
        "Graph 是由幾個主要組件組成的：\n",
        "\n",
        "1. 節點 (Nodes)：每個節點代表一個行動，像是呼叫 LLM、檢索資料、計算條件判斷等。\n",
        "\n",
        "2. 邊 (Edges)：節點與節點之間的連接，可以設定固定流程 (Next Step) 或依條件 (Condition) 分流。\n",
        "\n",
        "3. 狀態 (State)：在整個執行過程中，會保存並更新的狀態資訊，讓多輪互動成為可能。\n",
        "\n",
        "LangGraph 支援與 LangChain 模組緊密整合，例如：\n",
        "\n",
        "1. Prompt templates：在節點中組合提示內容。\n",
        "\n",
        "2. LLMs：調用 GPT-4o、Claude、Llama3 等模型進行回應。\n",
        "\n",
        "3. Tools & Agents：結合網路搜尋、資料庫查詢、計算器等外部工具。\n",
        "\n",
        "4. Memory：在流程中持續更新記憶體，實現更自然的多輪對話。"
      ],
      "metadata": {
        "id": "MCpc4SaqBnp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 以 LangGraph 做一個簡單的 ChatBot"
      ],
      "metadata": {
        "id": "7nqT7qK8F864"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 模型載入"
      ],
      "metadata": {
        "id": "fN9v_901JXSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "因示範用模型載入較大，所以這邊做了量化處理 (適用於低記憶體的 GPU，例如 Colab T4)。\n",
        "\n",
        "有野心的人可以召喚自己的魔法小卡來更換成更強大的閉源模型，或跟我一樣當免費仔使用其他開源模型。"
      ],
      "metadata": {
        "id": "SWA7I9M-CLjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 使用 4-bit 量化模型\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 與 4-bit 模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "u1XXfKgOPBph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立 text generation pipeline\n",
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")\n",
        "\n",
        "# 包裝成 LangChain 的 llm 物件\n",
        "llm = HuggingFacePipeline(pipeline=generator)"
      ],
      "metadata": {
        "id": "mM-tGIHEVpC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 定義狀態（State）"
      ],
      "metadata": {
        "id": "QGRsNkWfKyUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "核心概念\n",
        "1. 每個節點（Node）在執行前，會接收一個 State。\n",
        "\n",
        "2. 每個節點執行後，必須回傳更新後的 State。\n",
        "\n",
        "3. State 通常用 TypedDict 或 BaseModel 定義，且每個欄位應明確標注資料型別（如 str、list、dict）。\n",
        "\n",
        "> ※ 這裡只示範 TypedDict，對 BaseModel 有興趣的同學可以自行研究。\n",
        "\n",
        "常見 State 欄位類型\n",
        "1. query: str → 問題\n",
        "2. messages: Annotated[list, add_messages] → 聊天歷史訊息\n",
        "  \n",
        "  (messages 這個欄位是個列表，而且每次節點傳回新的 messages 時，不是覆蓋整個列表，而是append到原來的列表)\n",
        "3. docs: List[Document] → 找到的知識庫文件\n",
        "\n",
        "  (docs 這個欄位是個列表，裡面放的是 Document 類型的資料)\n",
        "4. error: Optional[str] → 記錄錯誤訊息（如果中途出錯）"
      ],
      "metadata": {
        "id": "nSpCQ8gvN3Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓下方定義 State 結構，只有一個欄位 messages，用來儲存對話訊息的歷史紀錄（人類訊息 + AI 回應）。"
      ],
      "metadata": {
        "id": "Df0osDsHKabL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]"
      ],
      "metadata": {
        "id": "s58YnIFM6cbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 定義節點（Node）"
      ],
      "metadata": {
        "id": "UuhGKXXHQWb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "每一個 Node 負責處理一個任務。\n",
        "\n",
        "這裡的 chatbot 節點，會接收使用者訊息，整理成 Breeze 格式的 prompt，丟給模型產生回應，然後把回應加回到對話紀錄中。"
      ],
      "metadata": {
        "id": "lndNZatRQoT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# 定義節點 (Node)\n",
        "def chatbot(state: State):\n",
        "    messages = state[\"messages\"]\n",
        "    if messages[-1].type != \"human\":\n",
        "        raise ValueError(\"最後一個訊息必須是 human（user）\")\n",
        "\n",
        "    # 第一次對話時，要求使用繁體中文(可有可無)\n",
        "    if len(messages) == 1:\n",
        "        user_message = messages[-1]\n",
        "        zh_hint = \"請使用繁體中文。\\n\"\n",
        "        patched_user_message = HumanMessage(content=zh_hint + user_message.content)\n",
        "        patched_messages = messages[:-1] + [patched_user_message]\n",
        "    else:\n",
        "        patched_messages = messages\n",
        "\n",
        "    chat_list = []\n",
        "    for m in patched_messages:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            chat_list.append({\"role\": \"user\", \"content\": m.content})\n",
        "        elif isinstance(m, AIMessage):\n",
        "            chat_list.append({\"role\": \"assistant\", \"content\": m.content})\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    chat_list.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        chat_list,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    result = llm.invoke(prompt)\n",
        "\n",
        "    return {\"messages\": messages + [AIMessage(content=result)]}"
      ],
      "metadata": {
        "id": "4FApvmOO6fDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 建立 LangGraph 流程圖（StateGraph）"
      ],
      "metadata": {
        "id": "499ivDniRFVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用 StateGraph 來把 chatbot 功能組成一個圖。\n",
        "\n",
        "這裡的流程非常單純，只有一個節點（chatbot），進入和結束的地方都設定在這個節點，並利用邊(edge)來連接。"
      ],
      "metadata": {
        "id": "m7Y8qpPDRISa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 建立 LangGraph\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "LamKYINR6g4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 視覺化"
      ],
      "metadata": {
        "id": "5jNsbfkUSUSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "KM4_GmMeWKKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. chatbot 結果"
      ],
      "metadata": {
        "id": "kwUTMo9LSXkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "這裡的 LangGraph chatbot 有「短期記憶」，但沒有「長期記憶」。\n",
        "\n",
        "messages 變數會持續累積每一輪對話，把過去對話都「重新包成 prompt」送進模型。模型每次回答前，都能「看到目前所有訊息」。\n",
        "\n",
        "後續的課程中將會講解並演示 Agent 的長期記憶 。"
      ],
      "metadata": {
        "id": "jiL7ARLDmjPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "messages = []\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    result = graph.invoke({\"messages\": messages})\n",
        "    response = result[\"messages\"][-1].content\n",
        "\n",
        "    print(\"AI 助理:\", response.strip())\n",
        "    messages.append(AIMessage(content=response))"
      ],
      "metadata": {
        "id": "FW0ow87u-9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tqeIx7mD6FUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 一、baseline"
      ],
      "metadata": {
        "id": "J_h70AyziyIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "將`關鍵字`比對換成`向量相似度`比對。"
      ],
      "metadata": {
        "id": "Si6Fh0ILsGou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 請將目前使用關鍵字比對的 route_by_query，改為使用向量相似度進行分類，並設一個合理的相似度門檻，根據檢索結果的分數判斷是否走 RAG 流程。  \n",
        "例如用向量相似度及自訂 threshold 決定要不要分到 retriever。"
      ],
      "metadata": {
        "id": "QBaG1NGfsIDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hint：similarity_search_with_score(...)  \n",
        "可參考去年的讀書會 R4：向量資料庫的基本操作"
      ],
      "metadata": {
        "id": "_9htWkfdsH2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 以 LangGraph 實現帶條件判斷的 RAG 流程"
      ],
      "metadata": {
        "id": "OCGdg2rLTN85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\"\n",
        "\n",
        "docs = [Document(page_content=txt.strip()) for txt in docs_text.strip().split(\"\\n\\n\")]\n",
        "\n",
        "# chromadb 預設使用的大型語言模型為 \"all-MiniLM-L6-v2\"，由於該大型語言模型不支持中文，所以將模型替換為 \"infgrad/stella-base-zh-v3-1792d\"，並對 embedding 進行量化\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "persist_path = \"document_store\"\n",
        "collection_name = \"naruto_collection\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_path,\n",
        "    collection_name=collection_name,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        ")"
      ],
      "metadata": {
        "id": "3ds8YDd2OPe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 模型載入"
      ],
      "metadata": {
        "id": "ZvNW71MfVQq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用的模型與 chatbot 教學同一顆"
      ],
      "metadata": {
        "id": "wFnvRVQpU7W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")"
      ],
      "metadata": {
        "id": "uD7juOvTU6to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 定義狀態（State）"
      ],
      "metadata": {
        "id": "myq54ykXoW0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "這裡定義的 RAGState，包含了查詢文字（query）、檢索到的文件（docs）、以及最後生成的回答（answer）。"
      ],
      "metadata": {
        "id": "BRCuRU3svPHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class RAGState(TypedDict):\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n"
      ],
      "metadata": {
        "id": "pphh39DEoYNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 定義節點（Node）"
      ],
      "metadata": {
        "id": "wpylZY_mo7QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    # similarity_search 距離越小越相似\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
        "\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    query, docs = state[\"query\"], state[\"docs\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output}\n",
        "\n",
        "def direct_generate_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output}\n",
        "\n",
        "# 定義 Route Node（決定走哪條路）\n",
        "def route_by_query(state):\n",
        "    #keywords = [\"火影\", \"忍者\", \"歷代\"]\n",
        "    query = state[\"query\"]\n",
        "    docs_vector = vectorstore.similarity_search_with_score(query)\n",
        "    #for i, (_, score) in enumerate(docs_vector):\n",
        "    print(f\"相似度：{docs_vector[0][1]:.4f}\")\n",
        "    choice = \"naruto\" if docs_vector[0][1] <= 0.8 else \"general\" #越小越相似（score = 0 表示完全一樣），介於0-2，通常在0-1.5之間\n",
        "    print(f\"跑到 → {choice}\")\n",
        "    return choice"
      ],
      "metadata": {
        "id": "Knra8WF3M0r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 建立 LangGraph 流程圖（StateGraph）"
      ],
      "metadata": {
        "id": "zTk67tnLpM_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 LangGraph / LangChain 裡，RunnableLambda 就是用來把一個普通的 Python 函式（function）包裝成一個「Runnable」物件。\n",
        "\n",
        "Runnable 是 LangChain 裡的一個「標準介面」（Protocol Interface），代表「這個東西可以 .invoke()、可以被執行」。\n",
        "\n",
        "> ※ 換句話說：RunnableLambda 讓普通函式能接到 LangGraph 的 Node 上運行。"
      ],
      "metadata": {
        "id": "G30TGyOVr-LV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "為什麼第一個例子 chatbot 不需要使用 RunnableLambda ?\n",
        "\n",
        "- 如果只是單線到底（直直連接），不管有幾個節點，都不用自己包 RunnableLambda。\n",
        "\n",
        "- 只要涉及「判斷分流」、「根據條件走不同路線」，就一定要自己把判斷的 function 包成 RunnableLambda"
      ],
      "metadata": {
        "id": "3mLuCxndtTxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(RAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "UGMAM1ZypQzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 視覺化"
      ],
      "metadata": {
        "id": "8I5pHFropsIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "2IxEyiWaNvd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. RAG 結果"
      ],
      "metadata": {
        "id": "BLWrWd1gVd6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    # 設定初始 State\n",
        "    init_state: RAGState = {\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "    # 呼叫 LangGraph\n",
        "    result = graph.invoke(init_state)\n",
        "    raw_output = result[\"answer\"]\n",
        "\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")"
      ],
      "metadata": {
        "id": "MeEqEVw2C88W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 二、advance"
      ],
      "metadata": {
        "id": "J_oJXt81jAX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "改成能支援多輪問答（Multi-turn RAG），並能根據前面的query判斷問題。"
      ],
      "metadata": {
        "id": "X70GyzdqsO-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 請將 RAGState 加入 history 欄位，並在生成回答時，將歷史對話與當前問題一起組成 prompt。"
      ],
      "metadata": {
        "id": "FWvjQZbIsO32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hint：\n",
        "```\n",
        "class MultiTurnRAGState(TypedDict):  \n",
        "    history: List[str]  \n",
        "    query: str  \n",
        "    docs: List[Document]  \n",
        "    answer: str\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "K8LYZIy5sOqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class MultiTurnRAGState(TypedDict):\n",
        "    history: List[str]\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str"
      ],
      "metadata": {
        "id": "383KJDIWjEOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query = state[\"query\"]\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\", \"history\": state[\"history\"]}\n",
        "\n",
        "def generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, docs, history = state[\"query\"], state[\"docs\"], state[\"history\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    history_text = \"\\n\".join([f\"使用者：{turn}\" for turn in history])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"歷史對話：\\n{history_text}\\n\\n\"\n",
        "        f\"內容：\\n{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output, \"history\": history}\n",
        "\n",
        "def direct_generate_node(state: MultiTurnRAGState) -> MultiTurnRAGState:\n",
        "    query, history = state[\"query\"], state[\"history\"]\n",
        "    history_text = \"\\n\".join([f\"使用者：{turn}\" for turn in history])\n",
        "    prompt = (\n",
        "        f\"請回答下列問題：\\n\\n\"\n",
        "        f\"歷史對話：\\n{history_text}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output, \"history\": history}\n",
        "\n",
        "# 定義 Route Node（決定走哪條路）\n",
        "def route_by_query(state):\n",
        "    query = state[\"query\"]\n",
        "    docs_vector = vectorstore.similarity_search_with_score(query)\n",
        "    #for i, (_, score) in enumerate(docs_vector):\n",
        "    print(f\"相似度：{docs_vector[0][1]:.4f}\")\n",
        "    choice = \"naruto\" if docs_vector[0][1] <= 0.8 else \"general\" #越小越相似（score = 0 表示完全一樣），介於0-2，通常在0-1.5之間\n",
        "    print(f\"跑到 → {choice}\")\n",
        "    return choice"
      ],
      "metadata": {
        "id": "vi1ZwzOBkUj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(MultiTurnRAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "c4KMbJmGkfZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "bDZr_C11kiah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history: List[str] = []\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \").strip()\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    # 設定初始 State\n",
        "    init_state: MultiTurnRAGState = {\n",
        "        \"history\": history.copy(),\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "    # 呼叫 LangGraph\n",
        "    result = graph.invoke(init_state)\n",
        "    raw_output = result[\"answer\"]\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")\n",
        "\n",
        "    # 將這一輪對話加入歷史\n",
        "    history.append(f\"使用者：{user_input}\")\n",
        "    history.append(f\"助手：{answer_text}\")"
      ],
      "metadata": {
        "id": "oHvzw2jIkoV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}